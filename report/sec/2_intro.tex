\section{Introduction}
\label{sec:introduction}

\subsection{Problem Background}

The sinking of the RMS Titanic on its maiden voyage represents not only a historical tragedy but also a rich data source for understanding survival patterns under extreme circumstances. The Kaggle competition "Titanic: Machine Learning from Disaster" challenges participants to predict passenger survival based on demographic and ticketing information. With 891 training samples and 418 test samples, each described by features including passenger class (Pclass), name, sex, age, number of siblings/spouses aboard (SibSp), number of parents/children aboard (Parch), ticket number, fare, cabin, and port of embarkation (Embarked), the dataset presents a classic binary classification task with realistic complications: missing values, categorical variables, and potential non-linear interactions between features.

The survival rate in the training set is approximately 38\%, indicating moderate class imbalance. Initial exploratory analysis reveals strong predictive signals: women and children had significantly higher survival rates due to the "women and children first" evacuation protocol, first-class passengers enjoyed better access to lifeboats, and fare prices correlated with both passenger class and survival likelihood. However, raw features alone provide insufficient discriminative power—sophisticated feature engineering is required to extract latent information encoded in passenger names (social titles), ticket numbers (group bookings), and cabin assignments (deck locations).

% INSERT FIGURE 1: Survival by Gender and Class
\begin{figure}[t]
    \centering
    \includegraphics[width=0.48\columnwidth]{figures/survival_by_sex.png}
    \includegraphics[width=0.48\columnwidth]{figures/survival_by_pclass.png}
    \caption{Exploratory analysis of survival patterns: (left) survival rate by gender showing strong bias toward female passengers (74.2\% vs 18.9\%); (right) survival rate by passenger class demonstrating socioeconomic advantage (63.0\% first class vs 24.2\% third class).}
    \label{fig:survival_patterns}
\end{figure}

% INSERT FIGURE 2: Age Distribution
\begin{figure}[t]
    \centering
    \includegraphics[width=0.95\columnwidth]{figures/age_distribution.png}
    \caption{Age distribution of passengers by survival status. Survivors show a slightly younger mean age (28.3 years) compared to non-survivors (30.6 years), with notable representation of children under 12 among survivors.}
    \label{fig:age_dist}
\end{figure}

\subsection{Our Approach}

In this work, we conduct a systematic investigation of feature engineering strategies and model selection for Titanic survival prediction. Our contributions are as follows:

\textbf{1. Comprehensive Feature Engineering Comparison:} We implement and compare three distinct feature engineering pipelines:
\begin{itemize}
    \item \textbf{RF (Random Forest-oriented):} Based on community best practices, featuring title simplification (Mr, Mrs, Miss, Master, Rare), family size aggregation, ticket prefix extraction, and cabin deck identification. Missing ages are imputed using a Random Forest regressor trained on available demographic features.
    \item \textbf{XGB (XGBoost-optimized):} Incorporates frequency encoding for high-cardinality categorical variables (ticket types, cabin assignments), interaction terms (Sex×Pclass, Pclass×AgeBin), and quartile-based fare binning to capture non-linear pricing effects.
    \item \textbf{MLP (Neural Network-ready):} Employs one-hot encoding for all categorical features with \texttt{drop\_first=True} to avoid multicollinearity, standardized continuous variables via StandardScaler, and carefully designed binning strategies for age and fare to assist gradient-based optimization.
\end{itemize}

\textbf{2. Rigorous Multi-Model Evaluation:} We train and evaluate six classical machine learning algorithms (Random Forest, Gradient Boosting, XGBoost, Logistic Regression, SVM, KNN) and one neural network (MLP) using consistent train-validation splits with stratified sampling. For tree-based methods, we perform grid search over key hyperparameters (number of estimators, learning rate, max depth, subsample ratio). For distance-based methods (SVM, KNN), we incorporate feature scaling within scikit-learn pipelines. All experiments are repeated across multiple random seeds (45, 2025, 777) to assess prediction stability and variance.

Our experimental results demonstrate that feature engineering choice has a more pronounced impact on performance than algorithmic selection within reasonable model families. The RF feature set paired with Random Forest classifier achieves the highest single-model validation accuracy of 79.67\%. Ablation studies reveal that title extraction and family size features contribute most significantly to predictive power, followed by strategic age imputation.