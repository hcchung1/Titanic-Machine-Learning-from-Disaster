\begin{abstract}
We investigate feature engineering strategies for Titanic survival prediction through systematic comparison of three pipelines: RF (community practices), XGB (gradient boosting optimized), and MLP (neural network ready). Testing seven algorithms across multiple random seeds, we find that thoughtful feature construction—particularly title extraction and family size derivation—matters more than model complexity. Our Random Forest with RF features achieves 79.67\% accuracy, ranking top 8\% on Kaggle. Results demonstrate that domain-informed feature engineering remains crucial for small tabular datasets, where tree-based methods outperform neural networks without extensive tuning.
\end{abstract}