\begin{abstract}
Survival prediction on the RMS Titanic dataset represents a fundamental binary classification challenge in machine learning, serving as an ideal benchmark for evaluating feature engineering strategies and model selection methodologies. In this work, we present a comprehensive investigation of multiple feature engineering pipelines and classification algorithms applied to the Kaggle Titanic competition. We systematically explore three distinct feature engineering approaches: a Random Forest-oriented strategy (RF) adapted from community best practices, an XGBoost-optimized feature set (XGB) incorporating frequency encoding and interaction terms, and a Multi-Layer Perceptron configuration (MLP) with standardized features and careful binning strategies. Our experimental framework encompasses six classical machine learning algorithms—Random Forest, Gradient Boosting, XGBoost, Logistic Regression, Support Vector Machines, and K-Nearest Neighbors—alongside a neural network approach. Through rigorous cross-validation across multiple random seeds, we identify Random Forest with the RF feature engineering pipeline as the optimal configuration, achieving a validation accuracy of 79.67\% and demonstrating robust generalization on the test set. Furthermore, we investigate ensemble strategies combining Random Forest and XGBoost across different random seeds, leveraging soft voting to enhance prediction stability. Our ablation studies reveal that sophisticated feature engineering, particularly title extraction from passenger names, family size derivation, and strategic handling of missing age values through RandomForestRegressor imputation, contributes substantially more to performance gains than algorithmic complexity alone. The findings underscore the critical importance of domain-informed feature construction in tabular data classification tasks and provide actionable insights for practitioners navigating similar structured prediction problems.
\end{abstract}